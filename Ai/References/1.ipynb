{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt to use Forecasting with the Temporal Fusion Transformer\n",
    "\n",
    "### Required resources: 128 GB of RAM is required to train four parquet files.\n",
    "\n",
    "- [Demand forecasting with the Temporal Fusion Transformer](https://pytorch-forecasting.readthedocs.io/en/stable/tutorials/stallion.html)\n",
    "- [Tommaso Guerrini - temporal-fusion-transformer-in-pytorch v12](https://www.kaggle.com/code/tomwarrens/temporal-fusion-transformer-in-pytorch?scriptVersionId=106300693)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T12:22:47.428961Z",
     "iopub.status.busy": "2024-12-31T12:22:47.428674Z",
     "iopub.status.idle": "2024-12-31T12:23:03.787572Z",
     "shell.execute_reply": "2024-12-31T12:23:03.786722Z",
     "shell.execute_reply.started": "2024-12-31T12:22:47.428933Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch-forecasting\n",
      "  Downloading pytorch_forecasting-1.2.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy<=3.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-forecasting) (1.26.4)\n",
      "Requirement already satisfied: torch!=2.0.1,<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-forecasting) (2.4.1+cu121)\n",
      "Collecting lightning<3.0.0,>=2.0.0 (from pytorch-forecasting)\n",
      "  Downloading lightning-2.5.0.post0-py3-none-any.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.4/40.4 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scipy<2.0,>=1.8 in /usr/local/lib/python3.10/dist-packages (from pytorch-forecasting) (1.13.1)\n",
      "Requirement already satisfied: pandas<3.0.0,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-forecasting) (2.1.4)\n",
      "Requirement already satisfied: scikit-learn<2.0,>=1.2 in /usr/local/lib/python3.10/dist-packages (from pytorch-forecasting) (1.2.2)\n",
      "Requirement already satisfied: PyYAML<8.0,>=5.4 in /usr/local/lib/python3.10/dist-packages (from lightning<3.0.0,>=2.0.0->pytorch-forecasting) (6.0.2)\n",
      "Requirement already satisfied: fsspec<2026.0,>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (2024.6.1)\n",
      "Requirement already satisfied: lightning-utilities<2.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from lightning<3.0.0,>=2.0.0->pytorch-forecasting) (0.11.9)\n",
      "Requirement already satisfied: packaging<25.0,>=20.0 in /usr/local/lib/python3.10/dist-packages (from lightning<3.0.0,>=2.0.0->pytorch-forecasting) (24.1)\n",
      "Requirement already satisfied: torchmetrics<3.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from lightning<3.0.0,>=2.0.0->pytorch-forecasting) (1.6.0)\n",
      "Requirement already satisfied: tqdm<6.0,>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from lightning<3.0.0,>=2.0.0->pytorch-forecasting) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<6.0,>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from lightning<3.0.0,>=2.0.0->pytorch-forecasting) (4.12.2)\n",
      "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.10/dist-packages (from lightning<3.0.0,>=2.0.0->pytorch-forecasting) (2.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0.0,>=1.3.0->pytorch-forecasting) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0.0,>=1.3.0->pytorch-forecasting) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0.0,>=1.3.0->pytorch-forecasting) (2024.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2.0,>=1.2->pytorch-forecasting) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2.0,>=1.2->pytorch-forecasting) (3.5.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch!=2.0.1,<3.0.0,>=2.0.0->pytorch-forecasting) (3.16.1)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=2.0.1,<3.0.0,>=2.0.0->pytorch-forecasting) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=2.0.1,<3.0.0,>=2.0.0->pytorch-forecasting) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=2.0.1,<3.0.0,>=2.0.0->pytorch-forecasting) (3.1.4)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (3.10.5)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities<2.0,>=0.10.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (71.0.4)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3.0.0,>=1.3.0->pytorch-forecasting) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=2.0.1,<3.0.0,>=2.0.0->pytorch-forecasting) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=2.0.1,<3.0.0,>=2.0.0->pytorch-forecasting) (1.3.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (1.11.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (4.0.3)\n",
      "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (3.10)\n",
      "Downloading pytorch_forecasting-1.2.0-py3-none-any.whl (181 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.9/181.9 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading lightning-2.5.0.post0-py3-none-any.whl (815 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m815.2/815.2 kB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: lightning, pytorch-forecasting\n",
      "Successfully installed lightning-2.5.0.post0 pytorch-forecasting-1.2.0\n",
      "Requirement already satisfied: pytorch-forecasting[mqf2] in /usr/local/lib/python3.10/dist-packages (1.2.0)\n",
      "Requirement already satisfied: numpy<=3.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-forecasting[mqf2]) (1.26.4)\n",
      "Requirement already satisfied: torch!=2.0.1,<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-forecasting[mqf2]) (2.4.1+cu121)\n",
      "Requirement already satisfied: lightning<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-forecasting[mqf2]) (2.5.0.post0)\n",
      "Requirement already satisfied: scipy<2.0,>=1.8 in /usr/local/lib/python3.10/dist-packages (from pytorch-forecasting[mqf2]) (1.13.1)\n",
      "Requirement already satisfied: pandas<3.0.0,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-forecasting[mqf2]) (2.1.4)\n",
      "Requirement already satisfied: scikit-learn<2.0,>=1.2 in /usr/local/lib/python3.10/dist-packages (from pytorch-forecasting[mqf2]) (1.2.2)\n",
      "Collecting cpflows (from pytorch-forecasting[mqf2])\n",
      "  Downloading cpflows-0.1.2.tar.gz (42 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.9/42.9 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: PyYAML<8.0,>=5.4 in /usr/local/lib/python3.10/dist-packages (from lightning<3.0.0,>=2.0.0->pytorch-forecasting[mqf2]) (6.0.2)\n",
      "Requirement already satisfied: fsspec<2026.0,>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting[mqf2]) (2024.6.1)\n",
      "Requirement already satisfied: lightning-utilities<2.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from lightning<3.0.0,>=2.0.0->pytorch-forecasting[mqf2]) (0.11.9)\n",
      "Requirement already satisfied: packaging<25.0,>=20.0 in /usr/local/lib/python3.10/dist-packages (from lightning<3.0.0,>=2.0.0->pytorch-forecasting[mqf2]) (24.1)\n",
      "Requirement already satisfied: torchmetrics<3.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from lightning<3.0.0,>=2.0.0->pytorch-forecasting[mqf2]) (1.6.0)\n",
      "Requirement already satisfied: tqdm<6.0,>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from lightning<3.0.0,>=2.0.0->pytorch-forecasting[mqf2]) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<6.0,>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from lightning<3.0.0,>=2.0.0->pytorch-forecasting[mqf2]) (4.12.2)\n",
      "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.10/dist-packages (from lightning<3.0.0,>=2.0.0->pytorch-forecasting[mqf2]) (2.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0.0,>=1.3.0->pytorch-forecasting[mqf2]) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0.0,>=1.3.0->pytorch-forecasting[mqf2]) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0.0,>=1.3.0->pytorch-forecasting[mqf2]) (2024.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2.0,>=1.2->pytorch-forecasting[mqf2]) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2.0,>=1.2->pytorch-forecasting[mqf2]) (3.5.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch!=2.0.1,<3.0.0,>=2.0.0->pytorch-forecasting[mqf2]) (3.16.1)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=2.0.1,<3.0.0,>=2.0.0->pytorch-forecasting[mqf2]) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=2.0.1,<3.0.0,>=2.0.0->pytorch-forecasting[mqf2]) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=2.0.1,<3.0.0,>=2.0.0->pytorch-forecasting[mqf2]) (3.1.4)\n",
      "Collecting backports.functools-lru-cache>=1.6.1 (from cpflows->pytorch-forecasting[mqf2])\n",
      "  Downloading backports.functools_lru_cache-2.0.0-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: cycler>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from cpflows->pytorch-forecasting[mqf2]) (0.12.1)\n",
      "Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.10/dist-packages (from cpflows->pytorch-forecasting[mqf2]) (1.0.0)\n",
      "Requirement already satisfied: kiwisolver>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from cpflows->pytorch-forecasting[mqf2]) (1.4.7)\n",
      "Requirement already satisfied: matplotlib>=3.0 in /usr/local/lib/python3.10/dist-packages (from cpflows->pytorch-forecasting[mqf2]) (3.7.1)\n",
      "Requirement already satisfied: pyparsing>=2.4.7 in /usr/local/lib/python3.10/dist-packages (from cpflows->pytorch-forecasting[mqf2]) (3.1.4)\n",
      "Requirement already satisfied: seaborn>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from cpflows->pytorch-forecasting[mqf2]) (0.12.2)\n",
      "Requirement already satisfied: six>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from cpflows->pytorch-forecasting[mqf2]) (1.16.0)\n",
      "Collecting subprocess32>=3.5.3 (from cpflows->pytorch-forecasting[mqf2])\n",
      "  Downloading subprocess32-3.5.4.tar.gz (97 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.4/97.4 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from cpflows->pytorch-forecasting[mqf2]) (0.19.1+cu121)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from cpflows->pytorch-forecasting[mqf2]) (3.11.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting[mqf2]) (3.10.5)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities<2.0,>=0.10.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting[mqf2]) (71.0.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->cpflows->pytorch-forecasting[mqf2]) (1.3.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->cpflows->pytorch-forecasting[mqf2]) (4.53.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->cpflows->pytorch-forecasting[mqf2]) (10.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=2.0.1,<3.0.0,>=2.0.0->pytorch-forecasting[mqf2]) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=2.0.1,<3.0.0,>=2.0.0->pytorch-forecasting[mqf2]) (1.3.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting[mqf2]) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting[mqf2]) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting[mqf2]) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting[mqf2]) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting[mqf2]) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting[mqf2]) (1.11.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting[mqf2]) (4.0.3)\n",
      "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting[mqf2]) (3.10)\n",
      "Downloading backports.functools_lru_cache-2.0.0-py2.py3-none-any.whl (6.7 kB)\n",
      "Building wheels for collected packages: cpflows, subprocess32\n",
      "  Building wheel for cpflows (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for cpflows: filename=cpflows-0.1.2-py3-none-any.whl size=54295 sha256=1db95db13d3242c654b15bfcedc049a98cc5bdc47c69d1c32b39fc7429752f01\n",
      "  Stored in directory: /root/.cache/pip/wheels/1b/c3/fd/cdcca84615eb336ec3d17d9d1fd858684240e91b47cf3dab6e\n",
      "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for subprocess32: filename=subprocess32-3.5.4-py3-none-any.whl size=6489 sha256=a20521c1301777e6f2fbdc9c90a2afaeff276cd49712f7aa5e8ca3f2877686ef\n",
      "  Stored in directory: /root/.cache/pip/wheels/64/19/61/d440ccd46a2a014bce61fc5c6c8495dedd32ef04cba8b34b28\n",
      "Successfully built cpflows subprocess32\n",
      "Installing collected packages: subprocess32, backports.functools-lru-cache, cpflows\n",
      "Successfully installed backports.functools-lru-cache-2.0.0 cpflows-0.1.2 subprocess32-3.5.4\n",
      "Collecting pytorch_optimizer\n",
      "  Downloading pytorch_optimizer-3.3.2-py3-none-any.whl.metadata (64 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.3/64.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>1.24.4 in /usr/local/lib/python3.10/dist-packages (from pytorch_optimizer) (1.26.4)\n",
      "Requirement already satisfied: torch>=1.10 in /usr/local/lib/python3.10/dist-packages (from pytorch_optimizer) (2.4.1+cu121)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->pytorch_optimizer) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->pytorch_optimizer) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->pytorch_optimizer) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->pytorch_optimizer) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->pytorch_optimizer) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->pytorch_optimizer) (2024.6.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10->pytorch_optimizer) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10->pytorch_optimizer) (1.3.0)\n",
      "Downloading pytorch_optimizer-3.3.2-py3-none-any.whl (211 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.6/211.6 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pytorch_optimizer\n",
      "Successfully installed pytorch_optimizer-3.3.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch-forecasting\n",
    "!pip install pytorch-forecasting[mqf2]\n",
    "!pip install pytorch_optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T12:23:03.788995Z",
     "iopub.status.busy": "2024-12-31T12:23:03.788581Z",
     "iopub.status.idle": "2024-12-31T12:23:11.501119Z",
     "shell.execute_reply": "2024-12-31T12:23:11.500354Z",
     "shell.execute_reply.started": "2024-12-31T12:23:03.788964Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ubuntu 22.04.3 LTS\n",
      "6.6.56+\n",
      "CONTAINER_NAME=None, BUILD_DATE=20241217-203356, CUDA=12.2.2\n",
      "               total        used        free      shared  buff/cache   available\n",
      "Mem:            31Gi       1.1Gi        23Gi       1.0Mi       7.1Gi        29Gi\n",
      "Swap:             0B          0B          0B\n",
      "My NVIDIA driver version is '560.35.03'.\n",
      "lrwxrwxrwx 1 root root   22 Nov 10  2023 cuda -> /etc/alternatives/cuda\n",
      "lrwxrwxrwx 1 root root   25 Nov 10  2023 cuda-12 -> /etc/alternatives/cuda-12\n",
      "drwxr-xr-x 1 root root 4096 Nov 10  2023 cuda-12.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Seed set to 2025\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import copy\n",
    "import random\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import lightning.pytorch as pln\n",
    "from lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor, RichProgressBar, TQDMProgressBar\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "\n",
    "from pytorch_forecasting import Baseline, TemporalFusionTransformer, TimeSeriesDataSet\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "from pytorch_forecasting.metrics import MAE, SMAPE, PoissonLoss, QuantileLoss\n",
    "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters\n",
    "\n",
    "sys.path.append(\"/kaggle/input/jane-street-real-time-market-data-forecasting\")\n",
    "import kaggle_evaluation.jane_street_inference_server\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  # avoid printing out absolute paths\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "#pd.options.display.max_rows = None\n",
    "\n",
    "!cat /etc/os-release | grep -oP \"PRETTY_NAME=\\\"\\K([^\\\"]*)\" && uname -r\n",
    "print(f\"CONTAINER_NAME={os.environ.get('CONTAINER_NAME',None)}, BUILD_DATE={os.environ.get('BUILD_DATE',None)}, CUDA={os.environ.get('CUDA_VERSION', None)}\")\n",
    "!free -h\n",
    "!nv_version=\"$(nvidia-smi --query-gpu=driver_version --format=csv,noheader)\" && echo \"My NVIDIA driver version is '${nv_version}'.\"\n",
    "!ls -l /usr/local | grep cuda\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    '''Sets the seed of the entire notebook so results are the same every time we run.\n",
    "    This is for REPRODUCIBILITY.'''\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    # When running on the CuDNN backend, two further options must be set\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    pln.seed_everything(seed)\n",
    "    # Set a fixed value for the hash seed\n",
    "\n",
    "def reduce_mem_usage(df, float16_as32=True):\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        if col_type != object and str(col_type)!='category':\n",
    "            c_min,c_max = df[col].min(),df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    if float16_as32:\n",
    "                        df[col] = df[col].astype(np.float32)\n",
    "                    else:\n",
    "                        df[col] = df[col].astype(np.float16)  \n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n",
    "set_seed(2025)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T12:23:11.503278Z",
     "iopub.status.busy": "2024-12-31T12:23:11.502314Z",
     "iopub.status.idle": "2024-12-31T12:23:21.272151Z",
     "shell.execute_reply": "2024-12-31T12:23:21.271287Z",
     "shell.execute_reply.started": "2024-12-31T12:23:11.503253Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block: 9, DateID: 1530 - 1698, TimeID: 000 - 967\n",
      "Test Data:(39, 85)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (3, 85)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>row_id</th><th>date_id</th><th>time_id</th><th>symbol_id</th><th>weight</th><th>is_scored</th><th>feature_00</th><th>feature_01</th><th>feature_02</th><th>feature_03</th><th>feature_04</th><th>feature_05</th><th>feature_06</th><th>feature_07</th><th>feature_08</th><th>feature_09</th><th>feature_10</th><th>feature_11</th><th>feature_12</th><th>feature_13</th><th>feature_14</th><th>feature_15</th><th>feature_16</th><th>feature_17</th><th>feature_18</th><th>feature_19</th><th>feature_20</th><th>feature_21</th><th>feature_22</th><th>feature_23</th><th>feature_24</th><th>feature_25</th><th>feature_26</th><th>feature_27</th><th>feature_28</th><th>feature_29</th><th>feature_30</th><th>&hellip;</th><th>feature_42</th><th>feature_43</th><th>feature_44</th><th>feature_45</th><th>feature_46</th><th>feature_47</th><th>feature_48</th><th>feature_49</th><th>feature_50</th><th>feature_51</th><th>feature_52</th><th>feature_53</th><th>feature_54</th><th>feature_55</th><th>feature_56</th><th>feature_57</th><th>feature_58</th><th>feature_59</th><th>feature_60</th><th>feature_61</th><th>feature_62</th><th>feature_63</th><th>feature_64</th><th>feature_65</th><th>feature_66</th><th>feature_67</th><th>feature_68</th><th>feature_69</th><th>feature_70</th><th>feature_71</th><th>feature_72</th><th>feature_73</th><th>feature_74</th><th>feature_75</th><th>feature_76</th><th>feature_77</th><th>feature_78</th></tr><tr><td>i64</td><td>i16</td><td>i16</td><td>i8</td><td>f32</td><td>bool</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f64</td><td>f64</td><td>f64</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>&hellip;</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td></tr></thead><tbody><tr><td>0</td><td>0</td><td>0</td><td>0</td><td>3.169998</td><td>true</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>null</td><td>-0.0</td><td>null</td><td>-0.0</td><td>-0.0</td><td>0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>-0.0</td><td>&hellip;</td><td>null</td><td>-0.0</td><td>null</td><td>-0.0</td><td>0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>null</td><td>0.0</td><td>null</td><td>null</td><td>-0.0</td><td>null</td><td>-0.0</td><td>0.0</td><td>null</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>0.0</td><td>-0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>null</td><td>null</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>-0.0</td></tr><tr><td>1</td><td>0</td><td>0</td><td>1</td><td>2.165993</td><td>true</td><td>0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>0.0</td><td>-0.0</td><td>null</td><td>-0.0</td><td>null</td><td>-0.0</td><td>-0.0</td><td>0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>-0.0</td><td>&hellip;</td><td>null</td><td>-0.0</td><td>null</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>null</td><td>0.0</td><td>null</td><td>null</td><td>-0.0</td><td>null</td><td>-0.0</td><td>0.0</td><td>null</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>0.0</td><td>-0.0</td><td>-0.0</td><td>0.0</td><td>-0.0</td><td>null</td><td>null</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>2</td><td>0</td><td>0</td><td>2</td><td>3.06555</td><td>true</td><td>0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>null</td><td>-0.0</td><td>null</td><td>-0.0</td><td>-0.0</td><td>0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>-0.0</td><td>&hellip;</td><td>null</td><td>-0.0</td><td>null</td><td>-0.0</td><td>0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>null</td><td>0.0</td><td>null</td><td>null</td><td>-0.0</td><td>null</td><td>-0.0</td><td>0.0</td><td>null</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>0.0</td><td>0.0</td><td>null</td><td>null</td><td>0.0</td><td>0.0</td><td>-0.0</td><td>-0.0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (3, 85)\n",
       "┌────────┬─────────┬─────────┬───────────┬───┬────────────┬────────────┬────────────┬────────────┐\n",
       "│ row_id ┆ date_id ┆ time_id ┆ symbol_id ┆ … ┆ feature_75 ┆ feature_76 ┆ feature_77 ┆ feature_78 │\n",
       "│ ---    ┆ ---     ┆ ---     ┆ ---       ┆   ┆ ---        ┆ ---        ┆ ---        ┆ ---        │\n",
       "│ i64    ┆ i16     ┆ i16     ┆ i8        ┆   ┆ f32        ┆ f32        ┆ f32        ┆ f32        │\n",
       "╞════════╪═════════╪═════════╪═══════════╪═══╪════════════╪════════════╪════════════╪════════════╡\n",
       "│ 0      ┆ 0       ┆ 0       ┆ 0         ┆ … ┆ 0.0        ┆ 0.0        ┆ -0.0       ┆ -0.0       │\n",
       "│ 1      ┆ 0       ┆ 0       ┆ 1         ┆ … ┆ 0.0        ┆ 0.0        ┆ 0.0        ┆ 0.0        │\n",
       "│ 2      ┆ 0       ┆ 0       ┆ 2         ┆ … ┆ 0.0        ┆ 0.0        ┆ -0.0       ┆ -0.0       │\n",
       "└────────┴─────────┴─────────┴───────────┴───┴────────────┴────────────┴────────────┴────────────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (6_274_576, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>date_id</th><th>time_id</th><th>time_idx</th></tr><tr><td>i64</td><td>i64</td><td>u32</td></tr></thead><tbody><tr><td>1530</td><td>0</td><td>0</td></tr><tr><td>1530</td><td>0</td><td>0</td></tr><tr><td>1530</td><td>0</td><td>0</td></tr><tr><td>1530</td><td>0</td><td>0</td></tr><tr><td>1530</td><td>0</td><td>0</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>1698</td><td>967</td><td>163591</td></tr><tr><td>1698</td><td>967</td><td>163591</td></tr><tr><td>1698</td><td>967</td><td>163591</td></tr><tr><td>1698</td><td>967</td><td>163591</td></tr><tr><td>1698</td><td>967</td><td>163591</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (6_274_576, 3)\n",
       "┌─────────┬─────────┬──────────┐\n",
       "│ date_id ┆ time_id ┆ time_idx │\n",
       "│ ---     ┆ ---     ┆ ---      │\n",
       "│ i64     ┆ i64     ┆ u32      │\n",
       "╞═════════╪═════════╪══════════╡\n",
       "│ 1530    ┆ 0       ┆ 0        │\n",
       "│ 1530    ┆ 0       ┆ 0        │\n",
       "│ 1530    ┆ 0       ┆ 0        │\n",
       "│ 1530    ┆ 0       ┆ 0        │\n",
       "│ 1530    ┆ 0       ┆ 0        │\n",
       "│ …       ┆ …       ┆ …        │\n",
       "│ 1698    ┆ 967     ┆ 163591   │\n",
       "│ 1698    ┆ 967     ┆ 163591   │\n",
       "│ 1698    ┆ 967     ┆ 163591   │\n",
       "│ 1698    ┆ 967     ┆ 163591   │\n",
       "│ 1698    ┆ 967     ┆ 163591   │\n",
       "└─────────┴─────────┴──────────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = None\n",
    "for i in range(6, 10):\n",
    "    train = pl.read_parquet(f\"/kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id={i}/part-0.parquet\")\n",
    "    print(f\"Block: {i}, DateID: {train['date_id'].min():04d} - {train['date_id'].max():04d}, TimeID: {train['time_id'].min():03d} - {train['time_id'].max():03d}\")\n",
    "    train = train.with_columns(\n",
    "        pl.col('date_id').cast(pl.Int64),\n",
    "        pl.col('time_id').cast(pl.Int64),\n",
    "    )\n",
    "    if df is None:\n",
    "        df = train\n",
    "    else:\n",
    "        df = df.vstack(train)\n",
    "\n",
    "del train\n",
    "_ = gc.collect()\n",
    "\n",
    "test_path = '/kaggle/input/jane-street-real-time-market-data-forecasting/test.parquet'\n",
    "test_df = pl.read_parquet(f\"{test_path}/date_id=0\")\n",
    "print(f\"Test Data:{test_df.shape}\")\n",
    "display(test_df.head(3))\n",
    "\n",
    "df_time_idx = df.group_by(['date_id', 'time_id'], maintain_order=True).all(\n",
    "    ).select(pl.col(['date_id', 'time_id']), pl.int_range(pl.len(), dtype=pl.UInt32).alias(\"time_idx\"))\n",
    "#display(df_time_idx.select(pl.col([\"date_id\", \"time_id\", 'time_idx'])))\n",
    "\n",
    "df = df.join(df_time_idx, on=[\"date_id\", \"time_id\"],  how=\"left\")\n",
    "display(df.select(pl.col([\"date_id\", \"time_id\", 'time_idx'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T12:23:21.273352Z",
     "iopub.status.busy": "2024-12-31T12:23:21.273089Z",
     "iopub.status.idle": "2024-12-31T12:23:21.279461Z",
     "shell.execute_reply": "2024-12-31T12:23:21.278680Z",
     "shell.execute_reply.started": "2024-12-31T12:23:21.273321Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if 1 == -1:\n",
    "    import seaborn as sns\n",
    "    import matplotlib\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    #corr_matrix = df.drop(['date_id','time_id']).corr()\n",
    "    corr_matrix = df[[f\"responder_{i}\" for i in range(9)]].corr()\n",
    "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "    colors = sns.color_palette('coolwarm', 16)\n",
    "    levels = np.linspace(-1, 1, 16)\n",
    "    cmap_plot, norm = matplotlib.colors.from_levels_and_colors(levels, colors, extend=\"max\")\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize = (10, 10))\n",
    "\n",
    "    mask_feature = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "    sns.heatmap(corr_matrix, \n",
    "        mask = mask_feature | (np.abs(corr_matrix) < 0.01),\n",
    "        annot=True, ax = ax, cbar=False,\n",
    "        cmap = cmap_plot, \n",
    "        norm = norm, annot_kws={\"size\": 13, \"color\": 'black'}\n",
    "    )\n",
    "\n",
    "    ax.hlines(range(corr_matrix.shape[1]), *ax.get_xlim(), color = 'black')\n",
    "    ax.vlines(range(corr_matrix.shape[1]), *ax.get_ylim(), color = 'black')\n",
    "\n",
    "    ax.set_title('Correlation Matrix between each time series: absolute values under 0.01 are masked', \n",
    "                fontsize = 20, color = 'black', fontweight = 'bold');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T12:23:21.282310Z",
     "iopub.status.busy": "2024-12-31T12:23:21.282012Z",
     "iopub.status.idle": "2024-12-31T12:23:52.539820Z",
     "shell.execute_reply": "2024-12-31T12:23:52.539078Z",
     "shell.execute_reply.started": "2024-12-31T12:23:21.282290Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df.shape:(6274576, 84)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_idx</th>\n",
       "      <th>date_id</th>\n",
       "      <th>symbol_id</th>\n",
       "      <th>weight</th>\n",
       "      <th>feature_00</th>\n",
       "      <th>feature_01</th>\n",
       "      <th>feature_02</th>\n",
       "      <th>feature_03</th>\n",
       "      <th>feature_04</th>\n",
       "      <th>feature_05</th>\n",
       "      <th>feature_06</th>\n",
       "      <th>feature_07</th>\n",
       "      <th>feature_08</th>\n",
       "      <th>feature_09</th>\n",
       "      <th>feature_10</th>\n",
       "      <th>feature_11</th>\n",
       "      <th>feature_12</th>\n",
       "      <th>feature_13</th>\n",
       "      <th>feature_14</th>\n",
       "      <th>feature_15</th>\n",
       "      <th>feature_16</th>\n",
       "      <th>feature_17</th>\n",
       "      <th>feature_18</th>\n",
       "      <th>feature_19</th>\n",
       "      <th>feature_20</th>\n",
       "      <th>feature_21</th>\n",
       "      <th>feature_22</th>\n",
       "      <th>feature_23</th>\n",
       "      <th>feature_24</th>\n",
       "      <th>feature_25</th>\n",
       "      <th>feature_26</th>\n",
       "      <th>feature_27</th>\n",
       "      <th>feature_28</th>\n",
       "      <th>feature_29</th>\n",
       "      <th>feature_30</th>\n",
       "      <th>feature_31</th>\n",
       "      <th>feature_32</th>\n",
       "      <th>feature_33</th>\n",
       "      <th>feature_34</th>\n",
       "      <th>feature_35</th>\n",
       "      <th>feature_36</th>\n",
       "      <th>feature_37</th>\n",
       "      <th>feature_38</th>\n",
       "      <th>feature_39</th>\n",
       "      <th>feature_40</th>\n",
       "      <th>feature_41</th>\n",
       "      <th>feature_42</th>\n",
       "      <th>feature_43</th>\n",
       "      <th>feature_44</th>\n",
       "      <th>feature_45</th>\n",
       "      <th>feature_46</th>\n",
       "      <th>feature_47</th>\n",
       "      <th>feature_48</th>\n",
       "      <th>feature_49</th>\n",
       "      <th>feature_50</th>\n",
       "      <th>feature_51</th>\n",
       "      <th>feature_52</th>\n",
       "      <th>feature_53</th>\n",
       "      <th>feature_54</th>\n",
       "      <th>feature_55</th>\n",
       "      <th>feature_56</th>\n",
       "      <th>feature_57</th>\n",
       "      <th>feature_58</th>\n",
       "      <th>feature_59</th>\n",
       "      <th>feature_60</th>\n",
       "      <th>feature_61</th>\n",
       "      <th>feature_62</th>\n",
       "      <th>feature_63</th>\n",
       "      <th>feature_64</th>\n",
       "      <th>feature_65</th>\n",
       "      <th>feature_66</th>\n",
       "      <th>feature_67</th>\n",
       "      <th>feature_68</th>\n",
       "      <th>feature_69</th>\n",
       "      <th>feature_70</th>\n",
       "      <th>feature_71</th>\n",
       "      <th>feature_72</th>\n",
       "      <th>feature_73</th>\n",
       "      <th>feature_74</th>\n",
       "      <th>feature_75</th>\n",
       "      <th>feature_76</th>\n",
       "      <th>feature_77</th>\n",
       "      <th>feature_78</th>\n",
       "      <th>responder_6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1530</td>\n",
       "      <td>0</td>\n",
       "      <td>3.084694</td>\n",
       "      <td>1.153571</td>\n",
       "      <td>1.563784</td>\n",
       "      <td>0.697396</td>\n",
       "      <td>0.756759</td>\n",
       "      <td>2.580965</td>\n",
       "      <td>0.171311</td>\n",
       "      <td>1.126353</td>\n",
       "      <td>0.536153</td>\n",
       "      <td>0.057150</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>76</td>\n",
       "      <td>-0.656288</td>\n",
       "      <td>2.110188</td>\n",
       "      <td>0.145784</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.203291</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.238222</td>\n",
       "      <td>-2.294707</td>\n",
       "      <td>-0.063560</td>\n",
       "      <td>-0.148218</td>\n",
       "      <td>1.721362</td>\n",
       "      <td>0.645580</td>\n",
       "      <td>1.477857</td>\n",
       "      <td>0.528492</td>\n",
       "      <td>1.153077</td>\n",
       "      <td>0.466157</td>\n",
       "      <td>0.145568</td>\n",
       "      <td>-0.546845</td>\n",
       "      <td>-0.694435</td>\n",
       "      <td>-0.163897</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.502917</td>\n",
       "      <td>0.910145</td>\n",
       "      <td>-0.507707</td>\n",
       "      <td>0.218792</td>\n",
       "      <td>0.412922</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.081268</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.023247</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.967165</td>\n",
       "      <td>0.262769</td>\n",
       "      <td>-0.426009</td>\n",
       "      <td>-3.682122</td>\n",
       "      <td>-1.549827</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.680807</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.786826</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.227900</td>\n",
       "      <td>0.044606</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.540213</td>\n",
       "      <td>-2.190280</td>\n",
       "      <td>0.385893</td>\n",
       "      <td>-0.460265</td>\n",
       "      <td>-0.415684</td>\n",
       "      <td>-0.457720</td>\n",
       "      <td>-1.333965</td>\n",
       "      <td>-2.234130</td>\n",
       "      <td>-0.352034</td>\n",
       "      <td>3.125156</td>\n",
       "      <td>0.493488</td>\n",
       "      <td>-0.959100</td>\n",
       "      <td>1.284456</td>\n",
       "      <td>-0.275493</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.188457</td>\n",
       "      <td>3.666236</td>\n",
       "      <td>0.848177</td>\n",
       "      <td>0.999516</td>\n",
       "      <td>3.071231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1530</td>\n",
       "      <td>1</td>\n",
       "      <td>2.232906</td>\n",
       "      <td>0.553354</td>\n",
       "      <td>1.730064</td>\n",
       "      <td>0.990195</td>\n",
       "      <td>0.611490</td>\n",
       "      <td>2.023031</td>\n",
       "      <td>0.319015</td>\n",
       "      <td>1.183371</td>\n",
       "      <td>0.562853</td>\n",
       "      <td>0.057789</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>76</td>\n",
       "      <td>-1.063518</td>\n",
       "      <td>1.037634</td>\n",
       "      <td>-0.255358</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.318528</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.466130</td>\n",
       "      <td>-2.160217</td>\n",
       "      <td>0.009386</td>\n",
       "      <td>0.042186</td>\n",
       "      <td>0.319811</td>\n",
       "      <td>0.143070</td>\n",
       "      <td>1.866907</td>\n",
       "      <td>1.238242</td>\n",
       "      <td>-1.986826</td>\n",
       "      <td>-0.476918</td>\n",
       "      <td>0.408439</td>\n",
       "      <td>-0.689795</td>\n",
       "      <td>-0.619278</td>\n",
       "      <td>0.081413</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.130648</td>\n",
       "      <td>0.726115</td>\n",
       "      <td>2.071485</td>\n",
       "      <td>0.179241</td>\n",
       "      <td>0.045131</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002134</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.828163</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.304763</td>\n",
       "      <td>0.870251</td>\n",
       "      <td>-0.095340</td>\n",
       "      <td>-0.888243</td>\n",
       "      <td>-0.159577</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.002680</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.736226</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.354893</td>\n",
       "      <td>1.309985</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.429267</td>\n",
       "      <td>-1.266970</td>\n",
       "      <td>0.385893</td>\n",
       "      <td>-0.248770</td>\n",
       "      <td>-0.286104</td>\n",
       "      <td>-0.455154</td>\n",
       "      <td>-1.797363</td>\n",
       "      <td>-2.535985</td>\n",
       "      <td>-0.734866</td>\n",
       "      <td>1.533782</td>\n",
       "      <td>0.033801</td>\n",
       "      <td>-0.960126</td>\n",
       "      <td>0.306505</td>\n",
       "      <td>-0.522036</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.138142</td>\n",
       "      <td>1.579439</td>\n",
       "      <td>0.179564</td>\n",
       "      <td>0.160609</td>\n",
       "      <td>1.979042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1530</td>\n",
       "      <td>2</td>\n",
       "      <td>2.404948</td>\n",
       "      <td>1.532503</td>\n",
       "      <td>2.095852</td>\n",
       "      <td>0.919688</td>\n",
       "      <td>0.583715</td>\n",
       "      <td>2.330047</td>\n",
       "      <td>0.337096</td>\n",
       "      <td>1.262236</td>\n",
       "      <td>0.496050</td>\n",
       "      <td>0.073556</td>\n",
       "      <td>81</td>\n",
       "      <td>2</td>\n",
       "      <td>59</td>\n",
       "      <td>-1.001967</td>\n",
       "      <td>1.105770</td>\n",
       "      <td>-0.304426</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.531873</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.301579</td>\n",
       "      <td>-1.615271</td>\n",
       "      <td>0.454406</td>\n",
       "      <td>-0.188808</td>\n",
       "      <td>0.015120</td>\n",
       "      <td>-0.159487</td>\n",
       "      <td>1.379064</td>\n",
       "      <td>0.604568</td>\n",
       "      <td>0.736194</td>\n",
       "      <td>0.522007</td>\n",
       "      <td>-0.183058</td>\n",
       "      <td>-0.632819</td>\n",
       "      <td>-0.839542</td>\n",
       "      <td>-0.209550</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.211059</td>\n",
       "      <td>0.788082</td>\n",
       "      <td>-0.575270</td>\n",
       "      <td>0.157013</td>\n",
       "      <td>0.178823</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.486033</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.121402</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.019831</td>\n",
       "      <td>0.741859</td>\n",
       "      <td>-1.735237</td>\n",
       "      <td>-0.707955</td>\n",
       "      <td>-0.510588</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.793936</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.191118</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.190607</td>\n",
       "      <td>1.381697</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.829545</td>\n",
       "      <td>-0.867858</td>\n",
       "      <td>0.385893</td>\n",
       "      <td>-0.295958</td>\n",
       "      <td>-0.386221</td>\n",
       "      <td>-0.345102</td>\n",
       "      <td>-1.598371</td>\n",
       "      <td>-2.111468</td>\n",
       "      <td>-0.780465</td>\n",
       "      <td>0.848857</td>\n",
       "      <td>-0.152994</td>\n",
       "      <td>-1.219395</td>\n",
       "      <td>0.359229</td>\n",
       "      <td>-0.636138</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.445388</td>\n",
       "      <td>0.300118</td>\n",
       "      <td>-0.043114</td>\n",
       "      <td>-0.065761</td>\n",
       "      <td>-0.506260</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   time_idx  date_id  symbol_id    weight  feature_00  feature_01  feature_02  \\\n",
       "0         0     1530          0  3.084694    1.153571    1.563784    0.697396   \n",
       "1         0     1530          1  2.232906    0.553354    1.730064    0.990195   \n",
       "2         0     1530          2  2.404948    1.532503    2.095852    0.919688   \n",
       "\n",
       "   feature_03  feature_04  feature_05  feature_06  feature_07  feature_08  \\\n",
       "0    0.756759    2.580965    0.171311    1.126353    0.536153    0.057150   \n",
       "1    0.611490    2.023031    0.319015    1.183371    0.562853    0.057789   \n",
       "2    0.583715    2.330047    0.337096    1.262236    0.496050    0.073556   \n",
       "\n",
       "   feature_09  feature_10  feature_11  feature_12  feature_13  feature_14  \\\n",
       "0          11           7          76   -0.656288    2.110188    0.145784   \n",
       "1          11           7          76   -1.063518    1.037634   -0.255358   \n",
       "2          81           2          59   -1.001967    1.105770   -0.304426   \n",
       "\n",
       "   feature_15  feature_16  feature_17  feature_18  feature_19  feature_20  \\\n",
       "0         0.0   -0.203291         0.0   -1.238222   -2.294707   -0.063560   \n",
       "1         0.0   -0.318528         0.0   -1.466130   -2.160217    0.009386   \n",
       "2         0.0   -0.531873         0.0   -1.301579   -1.615271    0.454406   \n",
       "\n",
       "   feature_21  feature_22  feature_23  feature_24  feature_25  feature_26  \\\n",
       "0   -0.148218    1.721362    0.645580    1.477857    0.528492    1.153077   \n",
       "1    0.042186    0.319811    0.143070    1.866907    1.238242   -1.986826   \n",
       "2   -0.188808    0.015120   -0.159487    1.379064    0.604568    0.736194   \n",
       "\n",
       "   feature_27  feature_28  feature_29  feature_30  feature_31  feature_32  \\\n",
       "0    0.466157    0.145568   -0.546845   -0.694435   -0.163897         0.0   \n",
       "1   -0.476918    0.408439   -0.689795   -0.619278    0.081413         0.0   \n",
       "2    0.522007   -0.183058   -0.632819   -0.839542   -0.209550         0.0   \n",
       "\n",
       "   feature_33  feature_34  feature_35  feature_36  feature_37  feature_38  \\\n",
       "0         0.0    0.502917    0.910145   -0.507707    0.218792    0.412922   \n",
       "1         0.0    1.130648    0.726115    2.071485    0.179241    0.045131   \n",
       "2         0.0    0.211059    0.788082   -0.575270    0.157013    0.178823   \n",
       "\n",
       "   feature_39  feature_40  feature_41  feature_42  feature_43  feature_44  \\\n",
       "0         0.0    0.081268         0.0         0.0   -2.023247         0.0   \n",
       "1         0.0    0.002134         0.0         0.0   -0.828163         0.0   \n",
       "2         0.0    0.486033         0.0         0.0   -1.121402         0.0   \n",
       "\n",
       "   feature_45  feature_46  feature_47  feature_48  feature_49  feature_50  \\\n",
       "0   -1.967165    0.262769   -0.426009   -3.682122   -1.549827         0.0   \n",
       "1   -1.304763    0.870251   -0.095340   -0.888243   -0.159577         0.0   \n",
       "2   -1.019831    0.741859   -1.735237   -0.707955   -0.510588         0.0   \n",
       "\n",
       "   feature_51  feature_52  feature_53  feature_54  feature_55  feature_56  \\\n",
       "0    0.680807         0.0         0.0   -2.786826         0.0   -1.227900   \n",
       "1   -0.002680         0.0         0.0   -1.736226         0.0   -2.354893   \n",
       "2    0.793936         0.0         0.0   -1.191118         0.0   -2.190607   \n",
       "\n",
       "   feature_57  feature_58  feature_59  feature_60  feature_61  feature_62  \\\n",
       "0    0.044606         0.0   -2.540213   -2.190280    0.385893   -0.460265   \n",
       "1    1.309985         0.0   -2.429267   -1.266970    0.385893   -0.248770   \n",
       "2    1.381697         0.0   -1.829545   -0.867858    0.385893   -0.295958   \n",
       "\n",
       "   feature_63  feature_64  feature_65  feature_66  feature_67  feature_68  \\\n",
       "0   -0.415684   -0.457720   -1.333965   -2.234130   -0.352034    3.125156   \n",
       "1   -0.286104   -0.455154   -1.797363   -2.535985   -0.734866    1.533782   \n",
       "2   -0.386221   -0.345102   -1.598371   -2.111468   -0.780465    0.848857   \n",
       "\n",
       "   feature_69  feature_70  feature_71  feature_72  feature_73  feature_74  \\\n",
       "0    0.493488   -0.959100    1.284456   -0.275493         0.0         0.0   \n",
       "1    0.033801   -0.960126    0.306505   -0.522036         0.0         0.0   \n",
       "2   -0.152994   -1.219395    0.359229   -0.636138         0.0         0.0   \n",
       "\n",
       "   feature_75  feature_76  feature_77  feature_78  responder_6  \n",
       "0    4.188457    3.666236    0.848177    0.999516     3.071231  \n",
       "1    1.138142    1.579439    0.179564    0.160609     1.979042  \n",
       "2    0.445388    0.300118   -0.043114   -0.065761    -0.506260  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Select and drop columns with 100% null values\n",
    "df = df.drop([col for col in df.columns if df.select(pl.col(col).null_count()).item() == df.height])\n",
    "\n",
    "# Select (if not provided) and drop columns with only one unique value\n",
    "bad_cols = [col for col in df.columns if df.select(pl.col(col).n_unique()).item() == 1]            \n",
    "df = df.drop(bad_cols)\n",
    "\n",
    "#for col in df.columns:\n",
    "#    # Set datatype for a numeric column as per the datatype of the first non-null item\n",
    "#    val = df.select(plr.col(col).drop_nulls().first()).item()\n",
    "#    #df = df.with_columns(plr.col(col).cast(plr.Int16) if isinstance(val, int) else plr.col(col).cast(plr.Float32))\n",
    "#    if isinstance(val, int):\n",
    "#        df = df.with_columns(plr.col(col).cast(plr.Float32))\n",
    "\n",
    "# Calculate None columns\n",
    "#display(df.select(pl.all().is_null().any()))\n",
    "\n",
    "# Calculate None rows\n",
    "#display(df.with_columns(null_count = pl.sum_horizontal(pl.all().is_null())))\n",
    "\n",
    "#df = df.fill_null(strategy=\"mean\")\n",
    "df = df.to_pandas()\n",
    "\n",
    "# Number of Nans\n",
    "#pd_df = df.to_pandas()\n",
    "#print(pd_df.shape)\n",
    "#print(pd_df.columns.to_list())\n",
    "#pd_df.isna().sum(axis = 0).rename('nans_per_column_train').rename_axis('column').reset_index().set_index('column')\n",
    "df = df.fillna(method='ffill').fillna(0)\n",
    "#df = df.fillna(0)\n",
    "\n",
    "feature_cols = []\n",
    "for col in df.columns:\n",
    "    if 'feature_' in col:\n",
    "        feature_cols.append(col)\n",
    "\n",
    "final_feature = ['time_idx', 'date_id', 'symbol_id', 'weight'] + feature_cols\n",
    "df = df[final_feature + ['responder_6']]\n",
    "#df = reduce_mem_usage(df, float16_as32=False)\n",
    "df['time_idx'] = df['time_idx'].astype(np.int32)\n",
    "df['date_id'] = df['date_id'].astype(np.int16)\n",
    "df['symbol_id'] = df['symbol_id'].astype(np.int8)\n",
    "print(f\"df.shape:{df.shape}\")\n",
    "display(df.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2024-12-31T12:24:40.500Z",
     "iopub.execute_input": "2024-12-31T12:23:52.541495Z",
     "iopub.status.busy": "2024-12-31T12:23:52.541247Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163491 100\n"
     ]
    }
   ],
   "source": [
    "#max_prediction_length = int(df['time_id'].max() // 5)\n",
    "max_prediction_length = 100\n",
    "max_encoder_length = max_prediction_length\n",
    "training_cutoff = int(df['time_idx'].max() - max_prediction_length)\n",
    "print(training_cutoff, max_prediction_length)\n",
    "\n",
    "training = TimeSeriesDataSet(\n",
    "    df[lambda x: x['time_idx'] <= training_cutoff],\n",
    "    time_idx = 'time_idx',\n",
    "    target = 'responder_6',\n",
    "    group_ids = ['symbol_id'],  # 'date_id'\n",
    "    #weight = ['weight'],\n",
    "    min_encoder_length=max_encoder_length // 4,  # keep encoder length long (as it is in the validation set)\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    min_prediction_length=1,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    static_categoricals=[],  # use a string type / categorified string\n",
    "    static_reals=['symbol_id'],\n",
    "    time_varying_known_categoricals=[],\n",
    "    time_varying_known_reals=['time_idx', 'date_id'],\n",
    "    variable_groups={},  # group of categorical variables can be treated as one variable\n",
    "    time_varying_unknown_categoricals=[],\n",
    "    time_varying_unknown_reals=feature_cols,\n",
    "    target_normalizer=GroupNormalizer(groups=['symbol_id'], transformation='softplus'),  # use softplus and normalize by group\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    "    allow_missing_timesteps=True,  # <--\n",
    ")\n",
    "\n",
    "# create validation set (predict=True) which means to predict the last max_prediction_length points in time\n",
    "# for each series\n",
    "validation = TimeSeriesDataSet.from_dataset(training, df, predict=True, stop_randomization=True)\n",
    "\n",
    "# create dataloaders for model\n",
    "batch_size = 128  # set this between 32 to 128\n",
    "train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0)\n",
    "val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size * 10, num_workers=0)\n",
    "\n",
    "#let's see how a naive model does\n",
    "\n",
    "actuals = torch.cat([y for x, (y, weight) in iter(val_dataloader)])\n",
    "baseline_predictions = Baseline().predict(val_dataloader)\n",
    "print(f\"{(actuals - baseline_predictions.cpu()).abs().mean().item():.4f}\")\n",
    "\n",
    "sm_loss = SMAPE().loss(actuals, baseline_predictions.cpu()).mean(axis = 1).median().item()\n",
    "print(f\"Median loss for naive prediction on validation: {sm_loss:.4f}\")\n",
    "print(training.get_parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2024-12-31T12:24:40.502Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# calculate baseline mean absolute error, i.e. predict next value as the last available value from the history\n",
    "baseline_predictions = Baseline().predict(val_dataloader, return_y=True)\n",
    "print(f\"{MAE()(baseline_predictions.output, baseline_predictions.y):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Temporal Fusion Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2024-12-31T12:24:40.502Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if 1 == -1:\n",
    "    # configure network and trainer\n",
    "    pln.seed_everything(42)\n",
    "    trainer = pln.Trainer(\n",
    "        accelerator=\"cpu\",\n",
    "        # clipping gradients is a hyperparameter and important to prevent divergance\n",
    "        # of the gradient for recurrent neural networks\n",
    "        gradient_clip_val=0.1,\n",
    "        #callbacks=[RichProgressBar()],  # <--\n",
    "    )\n",
    "\n",
    "    tft = TemporalFusionTransformer.from_dataset(\n",
    "        training,\n",
    "        # not meaningful for finding the learning rate but otherwise very important\n",
    "        learning_rate=0.03,\n",
    "        hidden_size=8,  # most important hyperparameter apart from learning rate\n",
    "        # number of attention heads. Set to up to 4 for large datasets\n",
    "        attention_head_size=1,\n",
    "        dropout=0.1,  # between 0.1 and 0.3 are good values\n",
    "        hidden_continuous_size=8,  # set to <= hidden_size\n",
    "        loss=QuantileLoss(),\n",
    "        optimizer=\"ranger\",\n",
    "        # reduce learning rate if no improvement in validation loss after x epochs\n",
    "        # reduce_on_plateau_patience=1000,\n",
    "    )\n",
    "    print(f\"Number of parameters in network: {tft.size() / 1e3:.1f}k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2024-12-31T12:24:40.502Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if 1 == -1:\n",
    "    # find optimal learning rate\n",
    "    from lightning.pytorch.tuner import Tuner\n",
    "\n",
    "    res = Tuner(trainer).lr_find(\n",
    "        tft,\n",
    "        train_dataloaders=train_dataloader,\n",
    "        val_dataloaders=val_dataloader,\n",
    "        max_lr=10.0,\n",
    "        min_lr=1e-6,\n",
    "        num_training=100,\n",
    "    )\n",
    "\n",
    "    print(f\"suggested learning rate: {res.suggestion()}\")\n",
    "    fig = res.plot(show=True, suggest=True)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2024-12-31T12:24:40.502Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from pytorch_forecasting.metrics import MultiHorizonMetric, MultiLoss, SMAPE\n",
    "from pytorch_forecasting.metrics.base_metrics import AggregationMetric\n",
    "import torch.nn as nn\n",
    "\n",
    "class R2Loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(R2Loss, self).__init__()\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        mse_loss = torch.sum((y_pred - y_true) ** 2)\n",
    "        var_y = torch.sum(y_true ** 2)\n",
    "        loss = mse_loss / (var_y + 1e-38)\n",
    "        return loss\n",
    "\n",
    "def r2_val(y_true, y_pred, sample_weight):\n",
    "    residuals = sample_weight * (y_true - y_pred) ** 2\n",
    "    weighted_residual_sum = np.sum(residuals)\n",
    "    # Calculate weighted sum of squared true values (denominator)\n",
    "    weighted_true_sum = np.sum(sample_weight * (y_true) ** 2)\n",
    "    # Calculate weighted R2\n",
    "    r2 = 1 - weighted_residual_sum / weighted_true_sum\n",
    "    return r2\n",
    "\n",
    "class R2LossMhm(MultiHorizonMetric):\n",
    "    def __init__(self):\n",
    "        super(R2LossMhm, self).__init__()\n",
    "\n",
    "    def loss(self, y_pred, target):\n",
    "        mse_loss = (y_pred.squeeze() - target) ** 2\n",
    "        var_y = target ** 2\n",
    "        loss = mse_loss / (var_y + 1e-38)\n",
    "        return loss\n",
    "\n",
    "class MAE(MultiHorizonMetric):\n",
    "    def loss(self, y_pred, target):\n",
    "        loss = (self.to_prediction(y_pred) - target).abs()\n",
    "        return loss\n",
    "\n",
    "class R2LossAgrM(AggregationMetric):\n",
    "    def __init__(self):\n",
    "        super(R2LossAgrM, self).__init__(metric=R2LossMhm())\n",
    "\n",
    "    def loss(self, y_pred, y_true):\n",
    "        mse_loss = torch.sum((y_pred - y_true) ** 2)\n",
    "        var_y = torch.sum(y_true ** 2)\n",
    "        loss = mse_loss / (var_y + 1e-38)\n",
    "        return loss\n",
    "\n",
    "class CustomProgressBar(TQDMProgressBar):\n",
    "    def init_validation_tqdm(self):\n",
    "        bar = super().init_validation_tqdm()\n",
    "        bar.set_description(\"running validation...\")\n",
    "        return bar\n",
    "\n",
    "# configure network and trainer\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=10, verbose=False, mode=\"min\")\n",
    "lr_logger = LearningRateMonitor()  # log the learning rate\n",
    "logger = TensorBoardLogger(\"lightning_logs\")  # logging results to a tensorboard\n",
    "\n",
    "trainer = pln.Trainer(\n",
    "    max_epochs=50,\n",
    "    accelerator=\"cpu\",\n",
    "    enable_model_summary=True,\n",
    "    gradient_clip_val=0.1,\n",
    "    limit_train_batches=50,  # coment in for training, running valiation every 30 batches\n",
    "    # fast_dev_run=True,  # comment in to check that networkor dataset has no serious bugs\n",
    "    callbacks=[lr_logger, early_stop_callback, RichProgressBar()],  # , CustomProgressBar()\n",
    "    logger=logger,\n",
    ")\n",
    "print(f\"logged_metrics: {trainer.logged_metrics}\\ntrainer.state: {trainer.state}\")\n",
    "\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    learning_rate=0.03,\n",
    "    hidden_size=16,\n",
    "    attention_head_size=2,\n",
    "    dropout=0.1,\n",
    "    hidden_continuous_size=8,\n",
    "    loss=QuantileLoss(),\n",
    "    #loss=MAE(),\n",
    "    #loss=MultiLoss(metrics=[MAE(), SMAPE()], weights=[2.0, 1.0]),\n",
    "    #loss=R2LossMhm(),\n",
    "    log_interval=10,  # uncomment for learning rate finder and otherwise, e.g. to 10 for logging every 10 batches\n",
    "    logging_metrics=[R2LossAgrM()],\n",
    "    optimizer=\"Ranger\",\n",
    "    reduce_on_plateau_patience=4,\n",
    ")\n",
    "print(f\"Number of parameters in network: {tft.size() / 1e3:.1f}k\")\n",
    "print(tft.hparams)\n",
    "\n",
    "# fit network\n",
    "trainer.fit(\n",
    "    tft,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2024-12-31T12:24:40.502Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# load the best model according to the validation loss\n",
    "# (given that we use early stopping, this is not necessarily the last epoch)\n",
    "best_model_path = trainer.checkpoint_callback.best_model_path\n",
    "best_tft = TemporalFusionTransformer.load_from_checkpoint(best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2024-12-31T12:24:40.502Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# calcualte mean absolute error on validation set\n",
    "predictions = best_tft.predict(val_dataloader, return_y=True, trainer_kwargs=dict(accelerator=\"cpu\"))\n",
    "MAE()(predictions.output, predictions.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2024-12-31T12:24:40.502Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if 1 == -1:\n",
    "    def r2_val(y_true, y_pred, sample_weight):\n",
    "        residuals = sample_weight * (y_true - y_pred) ** 2\n",
    "        weighted_residual_sum = np.sum(residuals)\n",
    "        # Calculate weighted sum of squared true values (denominator)\n",
    "        weighted_true_sum = np.sum(sample_weight * (y_true) ** 2)\n",
    "        # Calculate weighted R2\n",
    "        r2 = 1 - weighted_residual_sum / weighted_true_sum\n",
    "        return r2\n",
    "\n",
    "    predictions = best_tft.predict(val_dataloader, return_y=True, trainer_kwargs=dict(accelerator=\"cpu\"))\n",
    "    val_r2 = r2_val(predictions.y, predictions.output, weights_eval)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2024-12-31T12:24:40.503Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# raw predictions are a dictionary from which all kind of information including quantiles can be extracted\n",
    "raw_predictions = best_tft.predict(val_dataloader, mode=\"raw\", return_x=True)\n",
    "\n",
    "for idx in range(10):  # plot 10 examples\n",
    "    best_tft.plot_prediction(raw_predictions.x, raw_predictions.output, idx=idx, add_loss_to_title=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Worst performers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2024-12-31T12:24:40.503Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# calcualte metric by which to display\n",
    "predictions = best_tft.predict(val_dataloader, return_y=True)\n",
    "mean_losses = SMAPE(reduction=\"none\").loss(predictions.output, predictions.y[0]).mean(1)\n",
    "indices = mean_losses.argsort(descending=True)  # sort losses\n",
    "for idx in range(10):  # plot 10 examples\n",
    "    best_tft.plot_prediction(\n",
    "        raw_predictions.x,\n",
    "        raw_predictions.output,\n",
    "        idx=indices[idx],\n",
    "        add_loss_to_title=SMAPE(quantiles=best_tft.loss.quantiles),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actuals vs predictions by variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2024-12-31T12:24:40.503Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "predictions = best_tft.predict(val_dataloader, return_x=True)\n",
    "predictions_vs_actuals = best_tft.calculate_prediction_actual_by_variable(predictions.x, predictions.output)\n",
    "best_tft.plot_prediction_actual_by_variable(predictions_vs_actuals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on selected data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2024-12-31T12:24:40.503Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#best_tft.predict(\n",
    "#    training.filter(lambda x: (x.agency == \"Agency_01\") & (x.sku == \"SKU_01\") & (x.time_idx_first_prediction == 15)),\n",
    "#    mode=\"quantiles\",\n",
    "#)\n",
    "\n",
    "best_tft.predict(\n",
    "    training.filter(lambda x: (x.time_idx_first_prediction == 950)),\n",
    "    mode=\"quantiles\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 9871156,
     "sourceId": 84493,
     "sourceType": "competition"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
